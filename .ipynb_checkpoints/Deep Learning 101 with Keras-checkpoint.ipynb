{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning 101 - Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "125/125 [==============================] - 41s 325ms/step - loss: 0.7074 - acc: 0.5285 - val_loss: 0.6795 - val_acc: 0.6112\n",
      "Epoch 2/25\n",
      "125/125 [==============================] - 37s 300ms/step - loss: 0.6658 - acc: 0.6105 - val_loss: 0.6309 - val_acc: 0.6881\n",
      "Epoch 3/25\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.6341 - acc: 0.6565 - val_loss: 0.6034 - val_acc: 0.6578\n",
      "Epoch 4/25\n",
      "125/125 [==============================] - 37s 296ms/step - loss: 0.6120 - acc: 0.6735 - val_loss: 0.5882 - val_acc: 0.6806\n",
      "Epoch 5/25\n",
      "125/125 [==============================] - 38s 301ms/step - loss: 0.6000 - acc: 0.6915 - val_loss: 0.5380 - val_acc: 0.7113\n",
      "Epoch 6/25\n",
      "125/125 [==============================] - 37s 300ms/step - loss: 0.5772 - acc: 0.7150 - val_loss: 0.5280 - val_acc: 0.7437\n",
      "Epoch 7/25\n",
      "125/125 [==============================] - 38s 301ms/step - loss: 0.5649 - acc: 0.7140 - val_loss: 0.5985 - val_acc: 0.6881\n",
      "Epoch 8/25\n",
      "125/125 [==============================] - 38s 305ms/step - loss: 0.5391 - acc: 0.7200 - val_loss: 0.5260 - val_acc: 0.7462\n",
      "Epoch 9/25\n",
      "125/125 [==============================] - 37s 293ms/step - loss: 0.5418 - acc: 0.7470 - val_loss: 0.5243 - val_acc: 0.7311\n",
      "Epoch 10/25\n",
      "125/125 [==============================] - 37s 294ms/step - loss: 0.5193 - acc: 0.7500 - val_loss: 0.5373 - val_acc: 0.7225\n",
      "Epoch 11/25\n",
      "125/125 [==============================] - 37s 296ms/step - loss: 0.5172 - acc: 0.7595 - val_loss: 0.5310 - val_acc: 0.7487\n",
      "Epoch 12/25\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.4970 - acc: 0.7615 - val_loss: 0.4972 - val_acc: 0.7538\n",
      "Epoch 13/25\n",
      "125/125 [==============================] - 38s 304ms/step - loss: 0.4708 - acc: 0.7765 - val_loss: 0.6152 - val_acc: 0.7399\n",
      "Epoch 14/25\n",
      "125/125 [==============================] - 37s 293ms/step - loss: 0.5020 - acc: 0.7770 - val_loss: 0.5534 - val_acc: 0.7424\n",
      "Epoch 15/25\n",
      "125/125 [==============================] - 39s 311ms/step - loss: 0.4822 - acc: 0.7830 - val_loss: 0.5978 - val_acc: 0.7113\n",
      "Epoch 16/25\n",
      "125/125 [==============================] - 38s 305ms/step - loss: 0.4498 - acc: 0.7935 - val_loss: 0.5555 - val_acc: 0.7399\n",
      "Epoch 17/25\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.4615 - acc: 0.7935 - val_loss: 0.4760 - val_acc: 0.7626\n",
      "Epoch 18/25\n",
      "125/125 [==============================] - 38s 308ms/step - loss: 0.4599 - acc: 0.7955 - val_loss: 0.5550 - val_acc: 0.7538\n",
      "Epoch 19/25\n",
      "125/125 [==============================] - 38s 301ms/step - loss: 0.4507 - acc: 0.8010 - val_loss: 0.4875 - val_acc: 0.7828\n",
      "Epoch 20/25\n",
      "125/125 [==============================] - 37s 296ms/step - loss: 0.4444 - acc: 0.7935 - val_loss: 0.5744 - val_acc: 0.7762\n",
      "Epoch 21/25\n",
      "125/125 [==============================] - 38s 300ms/step - loss: 0.4472 - acc: 0.8000 - val_loss: 0.4551 - val_acc: 0.7790\n",
      "Epoch 22/25\n",
      "125/125 [==============================] - 38s 300ms/step - loss: 0.4617 - acc: 0.8020 - val_loss: 0.5100 - val_acc: 0.7563\n",
      "Epoch 23/25\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.4293 - acc: 0.8155 - val_loss: 0.4759 - val_acc: 0.7816\n",
      "Epoch 24/25\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.4203 - acc: 0.8015 - val_loss: 0.5344 - val_acc: 0.7399\n",
      "Epoch 25/25\n",
      "125/125 [==============================] - 37s 293ms/step - loss: 0.4253 - acc: 0.8090 - val_loss: 0.6208 - val_acc: 0.7863\n"
     ]
    }
   ],
   "source": [
    "'''This script goes along the blog post\n",
    "\"Building powerful image classification models using very little data\"\n",
    "from blog.keras.io.\n",
    "It uses data that can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "In our setup, we:\n",
    "- created a data/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 25\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "# Existem dois tipos de modelos, porém o Sequencial é o mais comum.\n",
    "\n",
    "#Aqui adicionamos uma camada convulacional à rede. Declaramos que ela deve extrair 32 filtros\n",
    "#e que os kernels serão uma matriz 3x3.\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "\n",
    "\n",
    "# Em seguida, uma camada de neurônios de ativação, RELU é um tipo de função de ativação.\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#A camada de Max Pooling é adicionada e tem como finalidade destacar apenas os filtros gerados \n",
    "#com maior valor de ativação. Ex.: Detectar Features mais relevantes\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Adicionamos mais camadas Convulacionais, de Ativação e de Max Pooling\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#Neste ponto, nosso modelo tem como output um mapa 3D de features! (Traços, Bordas, Retas) - Pequenas partes da imagem que \n",
    "#são comuns à cada classe.\n",
    "\n",
    "# Por fim, utilizamos a função Flatten, para converter nosso mapa 3D de Features em um Vetor de 1 Dimensão\n",
    "# (Pense em um array)\n",
    "# Adicionamos também as camadas Dense (Totalmente Conectadas) e mais funções de ativação ao modelo.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "#Para cada tipo de problema, existe uma loss function, uma função de otimização e as métricas mais adequadas.\n",
    "#Esses elementos são parâmetros para a compilação do modelo\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Após definir o modelo da rede e compilar, criamos instâncias de geradores de dados \n",
    "#(Elementos que vão ler cada uma das images de imput)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "classes = train_generator.class_indices\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, show=False):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(150, 150))\n",
    "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img_tensor[0])                           \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load model\n",
    "model = load_model(\"first_try.h5\")\n",
    "\n",
    "# image path\n",
    "img_path_1 = 'test/tchalla1.jpg'    # cat\n",
    "img_path_2 = 'test/dog1.jpg'    # dog\n",
    "\n",
    "# load a single image\n",
    "new_image_1 = load_image(img_path_1, True)\n",
    "new_image_2 = load_image(img_path_2, True)\n",
    "\n",
    "\n",
    "# check prediction\n",
    "pred_1 = model.predict(new_image_1)\n",
    "pred_2 = model.predict(new_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16985281]]\n",
      "[[0.9496022]]\n"
     ]
    }
   ],
   "source": [
    "print(pred_1)\n",
    "print(pred_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "if pred_1[0][0] >= 0.5:\n",
    "    result_1 = 'dog'\n",
    "else:\n",
    "    result_1 = 'cat'\n",
    "\n",
    "print(result_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "if pred_2[0][0] >= 0.5:\n",
    "    result_2 = 'dog'\n",
    "else:\n",
    "    result_2 = 'cat'\n",
    "\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
